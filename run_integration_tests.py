#!/usr/bin/env python3
"""
Claude Echo é›†æˆæµ‹è¯•æ‰§è¡Œå™¨
Integration Agent - è¿è¡Œæ‰€æœ‰é›†æˆæµ‹è¯•å¹¶ç”Ÿæˆç»¼åˆæŠ¥å‘Š

è¿™ä¸ªè„šæœ¬ä¼šæ‰§è¡Œä»¥ä¸‹æµ‹è¯•ï¼š
1. ç»¼åˆé›†æˆæµ‹è¯• (comprehensive_integration_test.py)
2. è‡ªåŠ¨åŒ–éƒ¨ç½²æµ‹è¯• (automated_deployment.py)
3. ç³»ç»Ÿå¥åº·ç›‘æ§æµ‹è¯• (system_health_monitor.py)
4. ç«¯åˆ°ç«¯åŠŸèƒ½éªŒè¯ (end_to_end_validation.py)
"""

import asyncio
import subprocess
import sys
import time
import json
from pathlib import Path
from typing import Dict, Any, List, Optional
from datetime import datetime
from dataclasses import dataclass
from enum import Enum


class TestStatus(Enum):
    NOT_STARTED = "not_started"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    SKIPPED = "skipped"


@dataclass
class TestResult:
    name: str
    status: TestStatus
    score: Optional[float] = None
    execution_time: Optional[float] = None
    output: Optional[str] = None
    error: Optional[str] = None


class IntegrationTestRunner:
    """é›†æˆæµ‹è¯•æ‰§è¡Œå™¨"""
    
    def __init__(self):
        self.test_results: Dict[str, TestResult] = {}
        self.start_time = datetime.now()
        
        # å®šä¹‰æµ‹è¯•å¥—ä»¶
        self.test_suite = {
            "comprehensive_integration": {
                "file": "comprehensive_integration_test.py",
                "name": "ç»¼åˆé›†æˆæµ‹è¯•",
                "description": "éªŒè¯æ‰€æœ‰ç»„ä»¶é—´çš„é›†æˆå’Œåä½œ",
                "timeout": 300,  # 5åˆ†é’Ÿ
                "required": True
            },
            "automated_deployment": {
                "file": "automated_deployment.py", 
                "name": "è‡ªåŠ¨åŒ–éƒ¨ç½²æµ‹è¯•",
                "description": "æµ‹è¯•ç³»ç»Ÿçš„è‡ªåŠ¨åŒ–éƒ¨ç½²å’Œå‡çº§èƒ½åŠ›",
                "timeout": 180,  # 3åˆ†é’Ÿ
                "required": False
            },
            "system_health_monitor": {
                "file": "system_health_monitor.py",
                "name": "ç³»ç»Ÿå¥åº·ç›‘æ§æµ‹è¯•", 
                "description": "éªŒè¯ç³»ç»Ÿå¥åº·ç›‘æ§å’Œæ€§èƒ½åŸºå‡†",
                "timeout": 180,  # 3åˆ†é’Ÿ
                "required": False
            },
            "end_to_end_validation": {
                "file": "end_to_end_validation.py",
                "name": "ç«¯åˆ°ç«¯åŠŸèƒ½éªŒè¯",
                "description": "å®Œæ•´çš„ç”¨æˆ·å­¦ä¹ ç”Ÿå‘½å‘¨æœŸæµ‹è¯•",
                "timeout": 240,  # 4åˆ†é’Ÿ
                "required": True
            }
        }
        
        print("ğŸš€ Claude Echo é›†æˆæµ‹è¯•æ‰§è¡Œå™¨åˆå§‹åŒ–å®Œæˆ")
    
    async def run_all_tests(self, selected_tests: Optional[List[str]] = None):
        """è¿è¡Œæ‰€æœ‰é›†æˆæµ‹è¯•"""
        print("ğŸ” å¼€å§‹æ‰§è¡Œ Claude Echo é›†æˆæµ‹è¯•å¥—ä»¶")
        print("=" * 70)
        
        # ç¡®å®šè¦è¿è¡Œçš„æµ‹è¯•
        tests_to_run = selected_tests or list(self.test_suite.keys())
        
        print(f"ğŸ“‹ æµ‹è¯•è®¡åˆ’: {len(tests_to_run)} ä¸ªæµ‹è¯•")
        for test_key in tests_to_run:
            test_info = self.test_suite[test_key]
            print(f"   - {test_info['name']}: {test_info['description']}")
        
        print("\n" + "-" * 70)
        
        # åˆå§‹åŒ–æµ‹è¯•ç»“æœ
        for test_key in tests_to_run:
            test_info = self.test_suite[test_key]
            self.test_results[test_key] = TestResult(
                name=test_info['name'],
                status=TestStatus.NOT_STARTED
            )
        
        # ä¸²è¡Œæ‰§è¡Œæµ‹è¯•ï¼ˆé¿å…èµ„æºå†²çªï¼‰
        for test_key in tests_to_run:
            await self._run_single_test(test_key)
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        await self._generate_comprehensive_report()
    
    async def _run_single_test(self, test_key: str):
        """è¿è¡Œå•ä¸ªæµ‹è¯•"""
        test_info = self.test_suite[test_key]
        test_result = self.test_results[test_key]
        
        print(f"\nğŸ” å¼€å§‹æµ‹è¯•: {test_info['name']}")
        print(f"   æ–‡ä»¶: {test_info['file']}")
        print(f"   è¶…æ—¶: {test_info['timeout']}ç§’")
        
        test_file = Path(test_info['file'])
        
        if not test_file.exists():
            test_result.status = TestStatus.SKIPPED
            test_result.error = f"æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_file}"
            print(f"   â­ï¸ è·³è¿‡æµ‹è¯•: æ–‡ä»¶ä¸å­˜åœ¨")
            return
        
        test_result.status = TestStatus.RUNNING
        start_time = time.time()
        
        try:
            # è¿è¡Œæµ‹è¯•
            print(f"   âš¡ æ‰§è¡Œä¸­...")
            
            process = await asyncio.create_subprocess_exec(
                sys.executable, str(test_file),
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                cwd=Path.cwd()
            )
            
            try:
                stdout, stderr = await asyncio.wait_for(
                    process.communicate(),
                    timeout=test_info['timeout']
                )
                
                execution_time = time.time() - start_time
                test_result.execution_time = execution_time
                
                if process.returncode == 0:
                    test_result.status = TestStatus.COMPLETED
                    test_result.output = stdout.decode('utf-8', errors='ignore')
                    
                    # å°è¯•ä»è¾“å‡ºä¸­æå–åˆ†æ•°
                    score = self._extract_score_from_output(test_result.output)
                    test_result.score = score
                    
                    print(f"   âœ… æµ‹è¯•å®Œæˆ - ç”¨æ—¶: {execution_time:.1f}ç§’" + 
                          (f", è¯„åˆ†: {score:.1%}" if score else ""))
                else:
                    test_result.status = TestStatus.FAILED
                    test_result.error = stderr.decode('utf-8', errors='ignore')
                    print(f"   âŒ æµ‹è¯•å¤±è´¥ - è¿”å›ç : {process.returncode}")
                    
            except asyncio.TimeoutError:
                test_result.status = TestStatus.FAILED
                test_result.error = f"æµ‹è¯•è¶…æ—¶ ({test_info['timeout']}ç§’)"
                print(f"   â° æµ‹è¯•è¶…æ—¶")
                
                # ç»ˆæ­¢è¿›ç¨‹
                try:
                    process.terminate()
                    await asyncio.wait_for(process.wait(), timeout=5.0)
                except:
                    process.kill()
                    
        except Exception as e:
            test_result.status = TestStatus.FAILED
            test_result.error = f"æµ‹è¯•æ‰§è¡Œå¼‚å¸¸: {str(e)}"
            test_result.execution_time = time.time() - start_time
            print(f"   âŒ æµ‹è¯•å¼‚å¸¸: {str(e)}")
    
    def _extract_score_from_output(self, output: str) -> Optional[float]:
        """ä»æµ‹è¯•è¾“å‡ºä¸­æå–è¯„åˆ†"""
        try:
            # æŸ¥æ‰¾å¸¸è§çš„è¯„åˆ†æ ¼å¼
            import re
            
            patterns = [
                r'æˆåŠŸç‡[:ï¼š]\s*(\d+(?:\.\d+)?)[%ï¼…]',
                r'ç»¼åˆè¯„åˆ†[:ï¼š]\s*(\d+(?:\.\d+)?)[%ï¼…]', 
                r'æ€»ä½“è¯„åˆ†[:ï¼š]\s*(\d+(?:\.\d+)?)[%ï¼…]',
                r'æ•´ä½“è¯„åˆ†[:ï¼š]\s*(\d+(?:\.\d+)?)[%ï¼…]',
                r'è¯„åˆ†[:ï¼š]\s*(\d+(?:\.\d+)?)/1\.0+',
                r'score[:ï¼š]\s*(\d+(?:\.\d+)?)'
            ]
            
            for pattern in patterns:
                match = re.search(pattern, output, re.IGNORECASE)
                if match:
                    score_str = match.group(1)
                    score = float(score_str)
                    
                    # å¦‚æœæ˜¯ç™¾åˆ†æ¯”æ ¼å¼ï¼Œè½¬æ¢ä¸ºå°æ•°
                    if '%' in match.group(0) or 'ï¼…' in match.group(0):
                        score = score / 100.0
                    elif '/1.0' in match.group(0):
                        pass  # å·²ç»æ˜¯0-1èŒƒå›´
                    elif score > 1.0:
                        score = score / 100.0  # å‡è®¾æ˜¯ç™¾åˆ†åˆ¶
                    
                    return min(1.0, max(0.0, score))
            
            return None
            
        except Exception:
            return None
    
    async def _generate_comprehensive_report(self):
        """ç”Ÿæˆç»¼åˆæµ‹è¯•æŠ¥å‘Š"""
        total_time = (datetime.now() - self.start_time).total_seconds()
        
        print("\n" + "="*90)
        print("ğŸ“Š Claude Echo æ™ºèƒ½å­¦ä¹ ç³»ç»Ÿ - ç»¼åˆé›†æˆæµ‹è¯•æŠ¥å‘Š")
        print("Integration Agent - æœ€ç»ˆç³»ç»Ÿé›†æˆéªŒè¯æŠ¥å‘Š")
        print("="*90)
        
        # æµ‹è¯•æ‰§è¡Œæ‘˜è¦
        total_tests = len(self.test_results)
        completed_tests = sum(1 for r in self.test_results.values() if r.status == TestStatus.COMPLETED)
        failed_tests = sum(1 for r in self.test_results.values() if r.status == TestStatus.FAILED)
        skipped_tests = sum(1 for r in self.test_results.values() if r.status == TestStatus.SKIPPED)
        
        print(f"\nğŸ“‹ æµ‹è¯•æ‰§è¡Œæ‘˜è¦:")
        print(f"   æ€»æµ‹è¯•æ•°: {total_tests}")
        print(f"   âœ… å®Œæˆ: {completed_tests}")
        print(f"   âŒ å¤±è´¥: {failed_tests}")
        print(f"   â­ï¸ è·³è¿‡: {skipped_tests}")
        print(f"   ğŸ“ˆ å®Œæˆç‡: {completed_tests/total_tests:.1%}")
        print(f"   â±ï¸ æ€»æ‰§è¡Œæ—¶é—´: {total_time:.1f}ç§’")
        
        # è¯¦ç»†æµ‹è¯•ç»“æœ
        print(f"\nğŸ“Š è¯¦ç»†æµ‹è¯•ç»“æœ:")
        
        for test_key, result in self.test_results.items():
            status_icons = {
                TestStatus.COMPLETED: "âœ…",
                TestStatus.FAILED: "âŒ",
                TestStatus.SKIPPED: "â­ï¸",
                TestStatus.NOT_STARTED: "â¸ï¸",
                TestStatus.RUNNING: "âš¡"
            }
            
            icon = status_icons.get(result.status, "â“")
            
            print(f"\n   {icon} {result.name}")
            print(f"      çŠ¶æ€: {result.status.value}")
            
            if result.execution_time:
                print(f"      æ‰§è¡Œæ—¶é—´: {result.execution_time:.1f}ç§’")
            
            if result.score is not None:
                print(f"      è¯„åˆ†: {result.score:.1%}")
            
            if result.error:
                print(f"      é”™è¯¯: {result.error[:100]}...")
            
            # æ˜¾ç¤ºå…³é”®è¾“å‡ºæ‘˜è¦
            if result.output and result.status == TestStatus.COMPLETED:
                key_metrics = self._extract_key_metrics(result.output)
                if key_metrics:
                    print(f"      å…³é”®æŒ‡æ ‡:")
                    for metric, value in key_metrics.items():
                        print(f"        - {metric}: {value}")
        
        # ç»¼åˆæ€§èƒ½è¯„ä¼°
        print(f"\nâš¡ ç³»ç»Ÿæ€§èƒ½è¯„ä¼°:")
        
        # è®¡ç®—ç»¼åˆè¯„åˆ†
        scored_tests = [r for r in self.test_results.values() if r.score is not None]
        if scored_tests:
            avg_score = sum(r.score for r in scored_tests) / len(scored_tests)
            print(f"   ğŸ“Š å¹³å‡è¯„åˆ†: {avg_score:.1%}")
        else:
            avg_score = 0.0
            print(f"   ğŸ“Š å¹³å‡è¯„åˆ†: æ— å¯ç”¨åˆ†æ•°")
        
        # æ€§èƒ½æŒ‡æ ‡
        total_exec_time = sum(r.execution_time for r in self.test_results.values() if r.execution_time)
        if total_exec_time > 0:
            print(f"   â±ï¸ æµ‹è¯•æ‰§è¡Œæ•ˆç‡: {total_tests/total_exec_time:.2f} æµ‹è¯•/ç§’")
        
        # ç³»ç»Ÿç¨³å®šæ€§è¯„ä¼°
        critical_failures = sum(1 for test_key, result in self.test_results.items() 
                               if result.status == TestStatus.FAILED and self.test_suite[test_key]['required'])
        
        stability_score = max(0.0, 1.0 - (critical_failures / max(1, total_tests)))
        print(f"   ğŸ›¡ï¸ ç³»ç»Ÿç¨³å®šæ€§: {stability_score:.1%}")
        
        # é›†æˆå°±ç»ªåº¦è¯„ä¼°
        print(f"\nğŸ¯ ç³»ç»Ÿé›†æˆå°±ç»ªåº¦è¯„ä¼°:")
        
        readiness_criteria = {
            "æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•é€šè¿‡": completed_tests >= total_tests * 0.8,
            "å…³é”®æµ‹è¯•æ— å¤±è´¥": critical_failures == 0,
            "å¹³å‡æ€§èƒ½è¾¾æ ‡": avg_score >= 0.75,
            "ç³»ç»Ÿç¨³å®šæ€§è‰¯å¥½": stability_score >= 0.8,
            "æµ‹è¯•æ‰§è¡Œé¡ºåˆ©": failed_tests <= total_tests * 0.2
        }
        
        passed_criteria = sum(readiness_criteria.values())
        total_criteria = len(readiness_criteria)
        readiness_score = passed_criteria / total_criteria
        
        for criteria, passed in readiness_criteria.items():
            icon = "âœ…" if passed else "âŒ"
            print(f"   {icon} {criteria}")
        
        print(f"\n   ğŸ“Š å°±ç»ªåº¦è¯„åˆ†: {readiness_score:.1%} ({passed_criteria}/{total_criteria})")
        
        # æœ€ç»ˆè¯„ä¼°å’Œå»ºè®®
        print(f"\nğŸ† æœ€ç»ˆç³»ç»Ÿè¯„ä¼°:")
        
        # è®¡ç®—ç»¼åˆè¯„ä¼°åˆ†æ•°
        final_score = (
            (completed_tests / total_tests) * 0.3 +  # æµ‹è¯•å®Œæˆåº¦
            avg_score * 0.4 +                        # å¹³å‡æ€§èƒ½åˆ†æ•°
            stability_score * 0.2 +                  # ç³»ç»Ÿç¨³å®šæ€§
            readiness_score * 0.1                    # å°±ç»ªåº¦
        )
        
        if final_score >= 0.9:
            verdict = "ğŸ‰ OUTSTANDING - ç³»ç»Ÿé›†æˆè´¨é‡å“è¶Šï¼Œå¯ç«‹å³éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ"
            recommendation = "ç³»ç»Ÿå·²å®Œå…¨å°±ç»ªï¼Œå»ºè®®ç«‹å³å¼€å§‹ç”¨æˆ·éªŒæ”¶æµ‹è¯•å’Œç”Ÿäº§éƒ¨ç½²"
        elif final_score >= 0.8:
            verdict = "âœ… EXCELLENT - ç³»ç»Ÿé›†æˆè´¨é‡ä¼˜ç§€ï¼Œå¯ä»¥éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ"
            recommendation = "ç³»ç»Ÿè¡¨ç°ä¼˜ç§€ï¼Œå»ºè®®åœ¨ä¿®å¤å°‘é‡é—®é¢˜åéƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ"
        elif final_score >= 0.7:
            verdict = "ğŸŸ¢ GOOD - ç³»ç»Ÿé›†æˆè´¨é‡è‰¯å¥½ï¼Œç»è¿‡å°å¹…ä¼˜åŒ–åå¯éƒ¨ç½²"
            recommendation = "ç³»ç»ŸåŸºæœ¬å°±ç»ªï¼Œå»ºè®®ä¿®å¤å·²è¯†åˆ«çš„é—®é¢˜åè¿›è¡Œéƒ¨ç½²"
        elif final_score >= 0.6:
            verdict = "ğŸŸ¡ ACCEPTABLE - ç³»ç»ŸåŸºæœ¬å¯ç”¨ï¼Œéœ€è¦ç»§ç»­ä¼˜åŒ–"
            recommendation = "ç³»ç»Ÿå¯ç”¨æ€§å¯æ¥å—ï¼Œå»ºè®®è§£å†³å…³é”®é—®é¢˜å¹¶é‡æ–°æµ‹è¯•"
        else:
            verdict = "ğŸ”´ NEEDS IMPROVEMENT - ç³»ç»Ÿéœ€è¦é‡å¤§æ”¹è¿›"
            recommendation = "ç³»ç»Ÿéœ€è¦å¤§å¹…æ”¹è¿›ï¼Œå»ºè®®ä¿®å¤æ‰€æœ‰å…³é”®é—®é¢˜åé‡æ–°è¿›è¡Œé›†æˆæµ‹è¯•"
        
        print(f"   ç»“æœ: {verdict}")
        print(f"   å»ºè®®: {recommendation}")
        print(f"   ğŸ“Š ç»¼åˆè¯„åˆ†: {final_score:.2f}/1.00 ({final_score*100:.1f}%)")
        
        # ä¸‹ä¸€æ­¥è¡ŒåŠ¨å»ºè®®
        print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥è¡ŒåŠ¨å»ºè®®:")
        
        if failed_tests > 0:
            print("   ğŸ”§ ä¼˜å…ˆä¿®å¤å¤±è´¥çš„æµ‹è¯•:")
            for test_key, result in self.test_results.items():
                if result.status == TestStatus.FAILED:
                    print(f"     - ä¿®å¤ {result.name}")
                    if result.error:
                        print(f"       é”™è¯¯: {result.error[:80]}...")
        
        if avg_score < 0.8:
            print("   ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–å»ºè®®:")
            print("     - æå‡ç³»ç»Ÿå“åº”é€Ÿåº¦")
            print("     - ä¼˜åŒ–å­¦ä¹ ç®—æ³•å‡†ç¡®ç‡")
            print("     - æ”¹å–„ç”¨æˆ·ä½“éªŒæŒ‡æ ‡")
        
        if readiness_score < 0.9:
            print("   ğŸ¯ å°±ç»ªåº¦æ”¹è¿›:")
            print("     - å®Œå–„ç³»ç»Ÿç›‘æ§æœºåˆ¶")
            print("     - å¼ºåŒ–é”™è¯¯å¤„ç†å’Œæ¢å¤")
            print("     - ä¼˜åŒ–èµ„æºä½¿ç”¨æ•ˆç‡")
        
        print("   ğŸ“‹ æŒç»­æ”¹è¿›:")
        print("     - å»ºç«‹æŒç»­é›†æˆ/éƒ¨ç½²æµç¨‹")
        print("     - å®æ–½ç”¨æˆ·åé¦ˆæ”¶é›†æœºåˆ¶")  
        print("     - å®šæœŸè¿›è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•")
        
        # ç”ŸæˆJSONæŠ¥å‘Š
        await self._save_json_report(final_score, readiness_score)
        
        print("\n" + "="*90)
        print(f"ğŸ¯ Claude Echo æ™ºèƒ½å­¦ä¹ ç³»ç»Ÿé›†æˆæµ‹è¯•å®Œæˆ!")
        print(f"ğŸ“Š ç³»ç»Ÿç»¼åˆè¯„åˆ†: {final_score:.1%}")
        print(f"ğŸ† {verdict.split(' - ')[0]}")
        print("="*90)
    
    def _extract_key_metrics(self, output: str) -> Dict[str, str]:
        """ä»è¾“å‡ºä¸­æå–å…³é”®æŒ‡æ ‡"""
        metrics = {}
        
        try:
            import re
            
            # æå–å¸¸è§æŒ‡æ ‡
            patterns = {
                'å‡†ç¡®ç‡': r'å‡†ç¡®ç‡[:ï¼š]\s*(\d+(?:\.\d+)?[%ï¼…]?)',
                'å“åº”æ—¶é—´': r'å“åº”æ—¶é—´[:ï¼š]\s*(\d+(?:\.\d+)?\s*[ç§’s]?)',
                'å†…å­˜ä½¿ç”¨': r'å†…å­˜[:ï¼š]\s*(\d+(?:\.\d+)?\s*MB)',
                'CPUä½¿ç”¨': r'CPU[:ï¼š]\s*(\d+(?:\.\d+)?[%ï¼…]?)',
                'ååé‡': r'ååé‡[:ï¼š]\s*(\d+(?:\.\d+)?)',
                'é”™è¯¯ç‡': r'é”™è¯¯ç‡[:ï¼š]\s*(\d+(?:\.\d+)?[%ï¼…]?)'
            }
            
            for metric_name, pattern in patterns.items():
                matches = re.findall(pattern, output, re.IGNORECASE)
                if matches:
                    metrics[metric_name] = matches[-1]  # å–æœ€åä¸€ä¸ªåŒ¹é…
            
        except Exception:
            pass
        
        return metrics
    
    async def _save_json_report(self, final_score: float, readiness_score: float):
        """ä¿å­˜JSONæ ¼å¼çš„è¯¦ç»†æŠ¥å‘Š"""
        report_data = {
            'timestamp': self.start_time.isoformat(),
            'execution_time_seconds': (datetime.now() - self.start_time).total_seconds(),
            'summary': {
                'total_tests': len(self.test_results),
                'completed_tests': sum(1 for r in self.test_results.values() if r.status == TestStatus.COMPLETED),
                'failed_tests': sum(1 for r in self.test_results.values() if r.status == TestStatus.FAILED),
                'skipped_tests': sum(1 for r in self.test_results.values() if r.status == TestStatus.SKIPPED),
                'average_score': sum(r.score for r in self.test_results.values() if r.score is not None) / max(1, len([r for r in self.test_results.values() if r.score is not None])),
                'final_score': final_score,
                'readiness_score': readiness_score
            },
            'test_results': {}
        }
        
        # æ·»åŠ è¯¦ç»†çš„æµ‹è¯•ç»“æœ
        for test_key, result in self.test_results.items():
            report_data['test_results'][test_key] = {
                'name': result.name,
                'status': result.status.value,
                'score': result.score,
                'execution_time': result.execution_time,
                'has_error': bool(result.error),
                'error_preview': result.error[:200] if result.error else None
            }
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = Path(f"integration_test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
        
        try:
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report_data, f, indent=2, ensure_ascii=False, default=str)
            
            print(f"\nâœ… è¯¦ç»†æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
            
        except Exception as e:
            print(f"\nâš ï¸ æŠ¥å‘Šä¿å­˜å¤±è´¥: {e}")
    
    def get_quick_status(self) -> str:
        """è·å–å¿«é€ŸçŠ¶æ€æ‘˜è¦"""
        completed = sum(1 for r in self.test_results.values() if r.status == TestStatus.COMPLETED)
        total = len(self.test_results)
        
        if completed == 0:
            return "ğŸ”„ æµ‹è¯•æœªå¼€å§‹"
        elif completed == total:
            return "âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆ"
        else:
            return f"âš¡ è¿›è¡Œä¸­ ({completed}/{total})"


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Claude Echo æ™ºèƒ½å­¦ä¹ ç³»ç»Ÿ - é›†æˆæµ‹è¯•å¥—ä»¶")
    print("Integration Agent - ç»¼åˆç³»ç»Ÿé›†æˆéªŒè¯")
    print("=" * 60)
    
    runner = IntegrationTestRunner()
    
    try:
        # å¯ä»¥åœ¨è¿™é‡Œé€‰æ‹©ç‰¹å®šçš„æµ‹è¯•
        # selected_tests = ["comprehensive_integration", "end_to_end_validation"]
        selected_tests = None  # None è¡¨ç¤ºè¿è¡Œæ‰€æœ‰æµ‹è¯•
        
        await runner.run_all_tests(selected_tests)
        
        print(f"\nğŸ‰ é›†æˆæµ‹è¯•å¥—ä»¶æ‰§è¡Œå®Œæˆ!")
        print(f"ğŸ“Š å¿«é€ŸçŠ¶æ€: {runner.get_quick_status()}")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸ é›†æˆæµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        print(f"ğŸ“Š å½“å‰çŠ¶æ€: {runner.get_quick_status()}")
    except Exception as e:
        print(f"\nâŒ é›†æˆæµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())