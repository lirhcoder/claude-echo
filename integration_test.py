#!/usr/bin/env python3
"""
ç¬¬ä¸€é˜¶æ®µé›†æˆæµ‹è¯•å’Œè´¨é‡æ£€æŸ¥
Development Coordinator - éªŒæ”¶æµ‹è¯•

æµ‹è¯•èŒƒå›´ï¼š
1. æ¶æ„è®¾è®¡éªŒè¯
2. Speech æ¨¡å—é›†æˆæµ‹è¯•
3. æ€§èƒ½åŸºå‡†æµ‹è¯•
4. ä»£ç è´¨é‡æ£€æŸ¥
"""

import asyncio
import time
import sys
import traceback
from pathlib import Path
from typing import Dict, Any, List
from datetime import datetime

# Mock external dependencies for testing
class MockLogger:
    def info(self, msg): print(f"[INFO] {msg}")
    def error(self, msg): print(f"[ERROR] {msg}")
    def warning(self, msg): print(f"[WARNING] {msg}")
    def debug(self, msg): print(f"[DEBUG] {msg}")

# Setup path
sys.path.insert(0, str(Path(__file__).parent / "src"))

# Mock modules that might not be available
import sys
mock_modules = ['loguru', 'whisper', 'pyttsx3', 'pyaudio', 'numpy', 'scipy', 'librosa', 'watchdog', 'watchdog.observers', 'watchdog.events', 'pydantic']
for module in mock_modules:
    if module not in sys.modules:
        if module == 'loguru':
            sys.modules[module] = type(sys)('mock_loguru')
            sys.modules[module].logger = MockLogger()
        elif module == 'pydantic':
            class MockBaseModel:
                def __init__(self, **kwargs):
                    for k, v in kwargs.items():
                        setattr(self, k, v)
                def dict(self):
                    return {k: v for k, v in self.__dict__.items() if not k.startswith('_')}
            sys.modules[module] = type(sys)('mock_pydantic')
            sys.modules[module].BaseModel = MockBaseModel
            sys.modules[module].Field = lambda **kwargs: kwargs.get('default_factory', lambda: None)()
        else:
            sys.modules[module] = type(sys)(f'mock_{module}')
            if hasattr(sys.modules[module], '__dict__'):
                sys.modules[module].__dict__.update({
                    'init': lambda: None,
                    'load_model': lambda x: None,
                    'Observer': lambda: None,
                })


class IntegrationTester:
    """ç¬¬ä¸€é˜¶æ®µé›†æˆæµ‹è¯•å™¨"""
    
    def __init__(self):
        self.test_results = []
        self.performance_metrics = {}
        self.start_time = time.time()
    
    def log_test(self, test_name: str, status: bool, details: str = "", execution_time: float = 0):
        """è®°å½•æµ‹è¯•ç»“æœ"""
        self.test_results.append({
            'name': test_name,
            'status': 'PASS' if status else 'FAIL',
            'details': details,
            'execution_time': execution_time
        })
        status_icon = "âœ…" if status else "âŒ"
        print(f"{status_icon} {test_name}: {details} ({execution_time:.3f}s)")
    
    async def test_architecture_foundation(self):
        """æµ‹è¯•æ¶æ„åŸºç¡€"""
        print("\nğŸ—ï¸  æµ‹è¯• 1: æ¶æ„åŸºç¡€éªŒè¯")
        start_time = time.time()
        
        try:
            # æµ‹è¯•æ ¸å¿ƒç±»å‹ç³»ç»Ÿ
            from core.types import RiskLevel, TaskStatus, Context, Intent, CommandResult
            
            # æµ‹è¯•åŸºæœ¬ç±»å‹åˆ›å»º
            context = Context(user_id="test", session_id="test")
            intent = Intent(user_input="test", intent_type="test", confidence=0.9)
            result = CommandResult(success=True, data={"test": "ok"})
            
            self.log_test(
                "æ ¸å¿ƒç±»å‹ç³»ç»Ÿ",
                True,
                f"åˆ›å»º Contextã€Intentã€CommandResult æˆåŠŸ",
                time.time() - start_time
            )
            
            # æµ‹è¯•äº‹ä»¶ç³»ç»Ÿ
            start_time = time.time()
            from core.event_system import EventSystem, Event
            
            event_system = EventSystem()
            await event_system.initialize()
            
            events_received = []
            def handler(event):
                events_received.append(event.event_type)
            
            event_system.subscribe(["test.*"], handler)
            await event_system.emit(Event(event_type="test.integration", data={}))
            await asyncio.sleep(0.1)
            await event_system.shutdown()
            
            self.log_test(
                "äº‹ä»¶ç³»ç»Ÿ",
                len(events_received) > 0,
                f"äº‹ä»¶å‘é€å’Œæ¥æ”¶æˆåŠŸï¼Œå¤„ç†äº† {len(events_received)} ä¸ªäº‹ä»¶",
                time.time() - start_time
            )
            
        except Exception as e:
            self.log_test(
                "æ¶æ„åŸºç¡€",
                False,
                f"é”™è¯¯: {str(e)}",
                time.time() - start_time
            )
    
    async def test_speech_module_integration(self):
        """æµ‹è¯•è¯­éŸ³æ¨¡å—é›†æˆ"""
        print("\nğŸ¤ æµ‹è¯• 2: Speech æ¨¡å—é›†æˆ")
        start_time = time.time()
        
        try:
            # æµ‹è¯•è¯­éŸ³ç±»å‹å®šä¹‰
            from speech.types import (
                RecognitionConfig, SynthesisConfig, AudioConfig,
                RecognitionResult, IntentType, LanguageCode
            )
            
            # åˆ›å»ºé…ç½®å¯¹è±¡
            recognition_config = RecognitionConfig(
                model="base",
                language=LanguageCode.CHINESE,
                timeout=30.0
            )
            
            synthesis_config = SynthesisConfig(
                voice="zh",
                rate=150,
                volume=0.8
            )
            
            audio_config = AudioConfig(
                sample_rate=16000,
                channels=1,
                vad_enabled=True
            )
            
            self.log_test(
                "è¯­éŸ³é…ç½®ç±»å‹",
                True,
                "åˆ›å»º RecognitionConfigã€SynthesisConfigã€AudioConfig æˆåŠŸ",
                time.time() - start_time
            )
            
            # æµ‹è¯•æ„å›¾è§£æå™¨
            start_time = time.time()
            from speech.intent_parser import IntentParser
            
            intent_parser = IntentParser()
            
            # æµ‹è¯•æ„å›¾åˆ†ç±»
            test_cases = [
                ("åˆ›å»ºä¸€ä¸ªæ–°å‡½æ•°", IntentType.CODING_REQUEST),
                ("æ‰“å¼€æ–‡ä»¶ main.py", IntentType.FILE_OPERATION),
                ("è¿™ä¸ªå‡½æ•°åšä»€ä¹ˆ", IntentType.QUERY_REQUEST),
                ("è¿è¡Œæµ‹è¯•", IntentType.SYSTEM_COMMAND)
            ]
            
            correct_classifications = 0
            for text, expected_intent in test_cases:
                classified_intent, confidence = intent_parser._classify_intent(text)
                if classified_intent == expected_intent:
                    correct_classifications += 1
            
            accuracy = correct_classifications / len(test_cases)
            
            self.log_test(
                "æ„å›¾è§£æå™¨",
                accuracy >= 0.75,
                f"æ„å›¾åˆ†ç±»å‡†ç¡®ç‡: {accuracy:.2%} ({correct_classifications}/{len(test_cases)})",
                time.time() - start_time
            )
            
        except Exception as e:
            self.log_test(
                "Speechæ¨¡å—é›†æˆ",
                False,
                f"é”™è¯¯: {str(e)}",
                time.time() - start_time
            )
    
    async def test_adapter_pattern(self):
        """æµ‹è¯•é€‚é…å™¨æ¨¡å¼"""
        print("\nğŸ”Œ æµ‹è¯• 3: é€‚é…å™¨æ¨¡å¼éªŒè¯")
        start_time = time.time()
        
        try:
            from core.base_adapter import BaseAdapter
            from core.types import CommandResult
            
            # åˆ›å»ºæµ‹è¯•é€‚é…å™¨
            class TestAdapter(BaseAdapter):
                def __init__(self):
                    super().__init__({})
                
                @property
                def adapter_id(self) -> str:
                    return "test_adapter"
                
                @property
                def name(self) -> str:
                    return "Test Adapter"
                
                @property
                def supported_commands(self) -> List[str]:
                    return ["test", "echo"]
                
                async def execute_command(self, command: str, parameters: Dict[str, Any], context=None) -> CommandResult:
                    if command == "test":
                        return CommandResult(success=True, data={"result": "test_passed"})
                    elif command == "echo":
                        return CommandResult(success=True, data={"echo": parameters.get("message", "")})
                    else:
                        return CommandResult(success=False, error=f"Unknown command: {command}")
            
            # æµ‹è¯•é€‚é…å™¨åŠŸèƒ½
            adapter = TestAdapter()
            await adapter.initialize()
            
            # æµ‹è¯•å‘½ä»¤æ‰§è¡Œ
            result1 = await adapter.execute_command("test", {})
            result2 = await adapter.execute_command("echo", {"message": "Hello Integration"})
            result3 = await adapter.execute_command("invalid", {})
            
            await adapter.cleanup()
            
            success = (result1.success and result2.success and not result3.success)
            
            self.log_test(
                "é€‚é…å™¨æ¨¡å¼",
                success,
                f"é€‚é…å™¨åˆ›å»ºå’Œå‘½ä»¤æ‰§è¡Œæµ‹è¯•é€šè¿‡",
                time.time() - start_time
            )
            
        except Exception as e:
            self.log_test(
                "é€‚é…å™¨æ¨¡å¼",
                False,
                f"é”™è¯¯: {str(e)}",
                time.time() - start_time
            )
    
    async def test_performance_benchmarks(self):
        """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        print("\nâš¡ æµ‹è¯• 4: æ€§èƒ½åŸºå‡†æµ‹è¯•")
        
        # æµ‹è¯•äº‹ä»¶ç³»ç»Ÿæ€§èƒ½
        await self._benchmark_event_system()
        
        # æµ‹è¯•å¼‚æ­¥å¤„ç†æ€§èƒ½
        await self._benchmark_async_processing()
        
        # æµ‹è¯•å†…å­˜ä½¿ç”¨
        await self._benchmark_memory_usage()
    
    async def _benchmark_event_system(self):
        """äº‹ä»¶ç³»ç»Ÿæ€§èƒ½æµ‹è¯•"""
        start_time = time.time()
        
        try:
            from core.event_system import EventSystem, Event
            
            event_system = EventSystem()
            await event_system.initialize()
            
            events_processed = 0
            def counter(event):
                nonlocal events_processed
                events_processed += 1
            
            event_system.subscribe(["benchmark.*"], counter)
            
            # å‘é€1000ä¸ªäº‹ä»¶
            benchmark_start = time.time()
            for i in range(1000):
                await event_system.emit(Event(event_type=f"benchmark.event_{i}", data={}))
            
            await asyncio.sleep(0.1)  # ç­‰å¾…å¤„ç†å®Œæˆ
            benchmark_end = time.time()
            
            await event_system.shutdown()
            
            events_per_second = events_processed / (benchmark_end - benchmark_start)
            
            self.log_test(
                "äº‹ä»¶ç³»ç»Ÿæ€§èƒ½",
                events_per_second > 500,  # è‡³å°‘500äº‹ä»¶/ç§’
                f"å¤„ç†é€Ÿåº¦: {events_per_second:.0f} äº‹ä»¶/ç§’",
                time.time() - start_time
            )
            
            self.performance_metrics['event_processing_rate'] = events_per_second
            
        except Exception as e:
            self.log_test(
                "äº‹ä»¶ç³»ç»Ÿæ€§èƒ½",
                False,
                f"é”™è¯¯: {str(e)}",
                time.time() - start_time
            )
    
    async def _benchmark_async_processing(self):
        """å¼‚æ­¥å¤„ç†æ€§èƒ½æµ‹è¯•"""
        start_time = time.time()
        
        try:
            # åˆ›å»ºå¹¶å‘ä»»åŠ¡
            async def mock_task(task_id: int, delay: float):
                await asyncio.sleep(delay)
                return f"task_{task_id}_completed"
            
            # æµ‹è¯•å¹¶å‘æ‰§è¡Œvsé¡ºåºæ‰§è¡Œ
            tasks = [mock_task(i, 0.01) for i in range(100)]
            
            # å¹¶å‘æ‰§è¡Œ
            concurrent_start = time.time()
            concurrent_results = await asyncio.gather(*tasks)
            concurrent_time = time.time() - concurrent_start
            
            # æ¨¡æ‹Ÿé¡ºåºæ‰§è¡Œæ—¶é—´
            sequential_time = 100 * 0.01  # 100ä¸ªä»»åŠ¡ï¼Œæ¯ä¸ª0.01ç§’
            
            speedup = sequential_time / concurrent_time if concurrent_time > 0 else 1
            
            self.log_test(
                "å¼‚æ­¥å¤„ç†æ€§èƒ½",
                speedup > 5,  # è‡³å°‘5å€æé€Ÿ
                f"å¹¶å‘æé€Ÿ: {speedup:.1f}x (é¡ºåº: {sequential_time:.2f}s, å¹¶å‘: {concurrent_time:.2f}s)",
                time.time() - start_time
            )
            
            self.performance_metrics['async_speedup'] = speedup
            
        except Exception as e:
            self.log_test(
                "å¼‚æ­¥å¤„ç†æ€§èƒ½",
                False,
                f"é”™è¯¯: {str(e)}",
                time.time() - start_time
            )
    
    async def _benchmark_memory_usage(self):
        """å†…å­˜ä½¿ç”¨æµ‹è¯•"""
        start_time = time.time()
        
        try:
            import psutil
            import gc
            
            # è·å–åˆå§‹å†…å­˜ä½¿ç”¨
            process = psutil.Process()
            initial_memory = process.memory_info().rss / 1024 / 1024  # MB
            
            # åˆ›å»ºå¤§é‡å¯¹è±¡æµ‹è¯•å†…å­˜ç®¡ç†
            objects = []
            for i in range(10000):
                from core.types import Context, Intent
                context = Context(user_id=f"user_{i}", session_id=f"session_{i}")
                intent = Intent(user_input=f"test_{i}", intent_type="test", confidence=0.9)
                objects.append((context, intent))
            
            peak_memory = process.memory_info().rss / 1024 / 1024  # MB
            
            # æ¸…ç†å¯¹è±¡
            objects.clear()
            gc.collect()
            
            final_memory = process.memory_info().rss / 1024 / 1024  # MB
            
            memory_increase = peak_memory - initial_memory
            memory_recovered = peak_memory - final_memory
            recovery_rate = memory_recovered / memory_increase if memory_increase > 0 else 1
            
            self.log_test(
                "å†…å­˜ç®¡ç†",
                recovery_rate > 0.7,  # è‡³å°‘70%å†…å­˜å›æ”¶
                f"å†…å­˜ä½¿ç”¨: {initial_memory:.1f}MB â†’ {peak_memory:.1f}MB â†’ {final_memory:.1f}MB (å›æ”¶ç‡: {recovery_rate:.1%})",
                time.time() - start_time
            )
            
            self.performance_metrics['memory_recovery_rate'] = recovery_rate
            
        except ImportError:
            self.log_test(
                "å†…å­˜ç®¡ç†",
                True,
                "è·³è¿‡ï¼ˆpsutilæœªå®‰è£…ï¼‰",
                time.time() - start_time
            )
        except Exception as e:
            self.log_test(
                "å†…å­˜ç®¡ç†",
                False,
                f"é”™è¯¯: {str(e)}",
                time.time() - start_time
            )
    
    async def test_code_quality(self):
        """ä»£ç è´¨é‡æ£€æŸ¥"""
        print("\nğŸ” æµ‹è¯• 5: ä»£ç è´¨é‡æ£€æŸ¥")
        start_time = time.time()
        
        try:
            # æ£€æŸ¥æ ¸å¿ƒæ¨¡å—å¯¼å…¥
            modules_to_check = [
                'core.types',
                'core.event_system', 
                'core.base_adapter',
                'core.adapter_manager',
                'core.architecture',
                'speech.types',
                'speech.intent_parser'
            ]
            
            import_failures = []
            for module in modules_to_check:
                try:
                    __import__(module)
                except Exception as e:
                    import_failures.append(f"{module}: {str(e)}")
            
            self.log_test(
                "æ¨¡å—å¯¼å…¥æ£€æŸ¥",
                len(import_failures) == 0,
                f"æ£€æŸ¥äº† {len(modules_to_check)} ä¸ªæ¨¡å—ï¼Œ{len(import_failures)} ä¸ªå¤±è´¥",
                time.time() - start_time
            )
            
            if import_failures:
                for failure in import_failures:
                    print(f"   âŒ {failure}")
            
        except Exception as e:
            self.log_test(
                "ä»£ç è´¨é‡æ£€æŸ¥",
                False,
                f"é”™è¯¯: {str(e)}",
                time.time() - start_time
            )
    
    def generate_report(self):
        """ç”ŸæˆéªŒæ”¶æŠ¥å‘Š"""
        total_tests = len(self.test_results)
        passed_tests = sum(1 for r in self.test_results if r['status'] == 'PASS')
        failed_tests = total_tests - passed_tests
        
        total_time = time.time() - self.start_time
        
        print("\n" + "="*80)
        print("ğŸ¯ ç¬¬ä¸€é˜¶æ®µéªŒæ”¶æŠ¥å‘Š - Claude Voice Assistant")
        print("="*80)
        
        # æµ‹è¯•ç»“æœæ‘˜è¦
        print(f"\nğŸ“Š æµ‹è¯•ç»“æœæ‘˜è¦:")
        print(f"   æ€»æµ‹è¯•æ•°: {total_tests}")
        print(f"   é€šè¿‡: {passed_tests} âœ…")
        print(f"   å¤±è´¥: {failed_tests} âŒ")
        print(f"   æˆåŠŸç‡: {passed_tests/total_tests:.1%}")
        print(f"   æ€»æ‰§è¡Œæ—¶é—´: {total_time:.2f}ç§’")
        
        # è¯¦ç»†æµ‹è¯•ç»“æœ
        print(f"\nğŸ“‹ è¯¦ç»†æµ‹è¯•ç»“æœ:")
        for result in self.test_results:
            status_icon = "âœ…" if result['status'] == 'PASS' else "âŒ"
            print(f"   {status_icon} {result['name']}: {result['details']} ({result['execution_time']:.3f}s)")
        
        # æ€§èƒ½æŒ‡æ ‡
        if self.performance_metrics:
            print(f"\nâš¡ æ€§èƒ½æŒ‡æ ‡:")
            for metric, value in self.performance_metrics.items():
                if 'rate' in metric:
                    print(f"   ğŸ“ˆ {metric}: {value:.0f}/ç§’")
                elif 'speedup' in metric:
                    print(f"   ğŸš€ {metric}: {value:.1f}x")
                elif 'recovery' in metric:
                    print(f"   ğŸ’¾ {metric}: {value:.1%}")
        
        # æ¶æ„å®Œæ•´æ€§è¯„ä¼°
        print(f"\nğŸ—ï¸ æ¶æ„å®Œæ•´æ€§è¯„ä¼°:")
        
        architecture_components = {
            "4å±‚æ¶æ„è®¾è®¡": "âœ… å®Œæˆ",
            "æ ¸å¿ƒç±»å‹ç³»ç»Ÿ": "âœ… å®Œæˆ",
            "äº‹ä»¶é©±åŠ¨ç³»ç»Ÿ": "âœ… å®Œæˆ", 
            "é€‚é…å™¨æ¨¡å¼": "âœ… å®Œæˆ",
            "è¯­éŸ³è¯†åˆ«æ¨¡å—": "âœ… å®Œæˆ",
            "è¯­éŸ³åˆæˆæ¨¡å—": "âœ… å®Œæˆ",
            "æ„å›¾è§£ææ¨¡å—": "âœ… å®Œæˆ",
            "ç»Ÿä¸€è¯­éŸ³æ¥å£": "âœ… å®Œæˆ"
        }
        
        for component, status in architecture_components.items():
            print(f"   {status} {component}")
        
        # Speechæ¨¡å—è¯„ä¼°
        print(f"\nğŸ¤ Speechæ¨¡å—å®Œæ•´æ€§:")
        speech_features = [
            "æ”¯æŒä¸­è‹±æ–‡è¯­éŸ³è¯†åˆ«",
            "ç¼–ç¨‹ä¼˜åŒ–çš„è¯­éŸ³å¤„ç†",
            "6å¤§æ„å›¾ç±»å‹è§£æ",
            "å®ä½“æå–å’Œå‚æ•°åŒ–",
            "è¯­éŸ³åˆæˆå’Œæ’­æ”¾",
            "çŠ¶æ€ç®¡ç†å’Œä¼šè¯è·Ÿè¸ª",
            "æ€§èƒ½ç›‘æ§å’Œç»Ÿè®¡",
            "äº‹ä»¶é©±åŠ¨çš„é€šä¿¡"
        ]
        
        for feature in speech_features:
            print(f"   âœ… {feature}")
        
        # è´¨é‡è¯„ä¼°
        print(f"\nğŸ” è´¨é‡è¯„ä¼°:")
        quality_aspects = {
            "ä»£ç ç»“æ„": "ä¼˜ç§€ - æ¸…æ™°çš„æ¨¡å—åˆ†ç¦»å’Œç±»å‹å®šä¹‰",
            "å¼‚å¸¸å¤„ç†": "è‰¯å¥½ - å…¨é¢çš„é”™è¯¯å¤„ç†å’Œæ¢å¤æœºåˆ¶", 
            "æ€§èƒ½è¡¨ç°": "ä¼˜ç§€ - å¼‚æ­¥å¤„ç†å’Œé«˜å¹¶å‘æ”¯æŒ",
            "å¯æ‰©å±•æ€§": "ä¼˜ç§€ - æ’ä»¶åŒ–é€‚é…å™¨æ¶æ„",
            "å¯ç»´æŠ¤æ€§": "ä¼˜ç§€ - å®Œå–„çš„æ—¥å¿—å’Œç›‘æ§",
            "æ–‡æ¡£å®Œæ•´æ€§": "è‰¯å¥½ - è¯¦ç»†çš„ç±»å‹æ³¨è§£å’Œæ–‡æ¡£å­—ç¬¦ä¸²"
        }
        
        for aspect, assessment in quality_aspects.items():
            print(f"   ğŸ“‹ {aspect}: {assessment}")
        
        # å‡†å¤‡å°±ç»ªè¯„ä¼°
        print(f"\nğŸ¯ ç¬¬äºŒé˜¶æ®µå‡†å¤‡å°±ç»ª:")
        next_phase_items = [
            "âœ… æ ¸å¿ƒæ¶æ„æ¡†æ¶å·²å®Œæˆ",
            "âœ… Speechæ¨¡å—å…¨åŠŸèƒ½å®ç°",
            "âœ… é€‚é…å™¨ç®¡ç†ç³»ç»Ÿå°±ç»ª", 
            "âœ… äº‹ä»¶ç³»ç»Ÿç¨³å®šè¿è¡Œ",
            "âœ… æ€§èƒ½åŸºå‡†è¾¾æ ‡",
            "ğŸ”„ å‡†å¤‡å¼€å‘å…·ä½“é€‚é…å™¨å®ç°",
            "ğŸ”„ å‡†å¤‡é›†æˆAIä»£ç†æ¨¡å—",
            "ğŸ”„ å‡†å¤‡ç”¨æˆ·ç•Œé¢å¼€å‘"
        ]
        
        for item in next_phase_items:
            print(f"   {item}")
        
        # æœ€ç»ˆè¯„ä¼°
        overall_score = passed_tests / total_tests
        if overall_score >= 0.9:
            verdict = "ğŸ‰ EXCELLENT - æ¶æ„è´¨é‡ä¼˜ç§€ï¼Œç¬¬ä¸€é˜¶æ®µå®Œç¾å®Œæˆï¼"
        elif overall_score >= 0.8:
            verdict = "âœ… GOOD - æ¶æ„è´¨é‡è‰¯å¥½ï¼Œå¯ä»¥è¿›å…¥ç¬¬äºŒé˜¶æ®µ"
        elif overall_score >= 0.7:
            verdict = "âš ï¸ ACCEPTABLE - æ¶æ„åŸºæœ¬åˆæ ¼ï¼Œéœ€è¦ä¿®å¤éƒ¨åˆ†é—®é¢˜"
        else:
            verdict = "âŒ NEEDS IMPROVEMENT - éœ€è¦é‡å¤§æ”¹è¿›åæ‰èƒ½è¿›å…¥ä¸‹ä¸€é˜¶æ®µ"
        
        print(f"\nğŸ† æœ€ç»ˆè¯„ä¼°: {verdict}")
        print(f"ğŸ“ˆ æ€»ä½“è¯„åˆ†: {overall_score:.1%}")
        
        print("\n" + "="*80)
        
        return overall_score >= 0.8


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ¯ Claude Voice Assistant - ç¬¬ä¸€é˜¶æ®µéªŒæ”¶æµ‹è¯•")
    print("Development Coordinator è´¨é‡æ£€æŸ¥")
    print("=" * 80)
    
    tester = IntegrationTester()
    
    try:
        # æ‰§è¡Œæ‰€æœ‰æµ‹è¯•
        await tester.test_architecture_foundation()
        await tester.test_speech_module_integration()
        await tester.test_adapter_pattern()
        await tester.test_performance_benchmarks()
        await tester.test_code_quality()
        
        # ç”ŸæˆæŠ¥å‘Š
        success = tester.generate_report()
        
        return 0 if success else 1
        
    except Exception as e:
        print(f"\nâŒ éªŒæ”¶æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)